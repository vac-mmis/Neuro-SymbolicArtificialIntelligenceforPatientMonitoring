{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and data statistics\n",
    "In this notebook you can get more information about the statistics of the data. You can have a look on the MNIST data as well as on the generated data for the subtraction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('/app/dpl/')\n",
    "from data.pre_processing import MNISTTrain, MNISTest, MNISTDiffTrain, MNISTDiffTest\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for MNIST subtraction\n",
    "For the MNIST subtraction we first generated combinations of the MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We generate 59999 samples for the training set and 1998 samples for the test set.\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_train = MNISTTrain(transform=transform)\n",
    "mnist_test = MNISTest(transform=transform)\n",
    "mnist_test.drop_samples(0.8)\n",
    "\n",
    "# generate MNIST pairs and their corresponding labels\n",
    "diff_train = MNISTDiffTrain(mnist_train)\n",
    "diff_test = MNISTDiffTest(mnist_test)\n",
    "print(f'We generate {len(diff_train)} samples for the training set and {len(diff_test)} samples for the test set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the distribution of classes in the training set. The further we are from zero, the fewer samples we have for the corresponding classes. This results from the fact that we have fewer possible combinations which result in class 9 (9-0) then for example for class 8 (9-1, 8-0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_label\n",
      " 0    5850\n",
      " 1    5811\n",
      "-1    5513\n",
      "-2    4790\n",
      " 2    4676\n",
      "-3    4110\n",
      " 3    4079\n",
      " 4    3536\n",
      "-4    3506\n",
      " 5    2910\n",
      "-5    2886\n",
      " 6    2523\n",
      "-6    2421\n",
      "-7    1902\n",
      " 7    1858\n",
      "-8    1216\n",
      " 8    1194\n",
      "-9     628\n",
      " 9     590\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "diff_train.print_data_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above we have several combinations which can result in a specific class. Here you can see the number of samples per combination for class 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x1_label  x2_label  count\n",
      "0         0         1    714\n",
      "1         1         2    701\n",
      "2         2         3    645\n",
      "3         3         4    652\n",
      "4         4         5    548\n",
      "5         5         6    607\n",
      "6         6         7    650\n",
      "7         7         8    638\n",
      "8         8         9    656\n"
     ]
    }
   ],
   "source": [
    "diff_train.print_class_statistics(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test how sample efficient the neural based and NeSy approach are, we reduced the number of combinations per class to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of class 1 and its combinations:\n",
      "   x1_label  x2_label  count\n",
      "0         0         1    714\n",
      "\n",
      "Statistics of the overall training dataset:\n",
      "class_label\n",
      "6     726\n",
      "1     714\n",
      "-1    689\n",
      "-2    671\n",
      "-8    639\n",
      "-5    638\n",
      "-9    628\n",
      "-7    606\n",
      "-3    603\n",
      "7     602\n",
      "9     590\n",
      "5     576\n",
      "-4    569\n",
      "8     557\n",
      "-6    556\n",
      "0     555\n",
      "2     552\n",
      "4     535\n",
      "3     524\n",
      "Name: count, dtype: int64\n",
      "\n",
      "We reduced the number of samples for the original dataset (59999 samples) to only 11530 samples.\n"
     ]
    }
   ],
   "source": [
    "diff_train.set_num_class_samples(1)\n",
    "print('Statistics of class 1 and its combinations:')\n",
    "diff_train.print_class_statistics(1)\n",
    "print('\\nStatistics of the overall training dataset:')\n",
    "diff_train.print_data_statistics()\n",
    "print(f'\\nWe reduced the number of samples for the original dataset (59999 samples) to only {len(diff_train)} samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for MNIST task\n",
    "For the pretraining of the LeNet we used only 50% of the available MNIST data, because it is sufficient to get to a satisfying result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has 30000 samples. Samples are (approximately) evenly distributed between the different classes.\n",
      "The test set has 1999 samples. Samples are (approximately) evenly distributed between the different classes.\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_train = MNISTTrain(transform=transform)\n",
    "mnist_train.drop_samples(0.5)\n",
    "mnist_test = MNISTest(transform=transform)\n",
    "mnist_test.drop_samples(0.8)\n",
    "\n",
    "print(f'The training set has {len(mnist_train)} samples. Samples are (approximately) evenly distributed between the different classes.')\n",
    "print(f'The test set has {len(mnist_test)} samples. Samples are (approximately) evenly distributed between the different classes.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
